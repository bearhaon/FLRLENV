{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPivZHGwY3co",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Hyperparameters\n",
        "\n",
        "RL_ALGORITHM_NAME = 'TD3' #@param [\"TD3\", \"DDPG\", \"SAC\", \"PPO\"]\n",
        "FL_MODEL_NAME = 'EMNIST' #@param [\"EMNIST\", \"Shakespeare\"]\n",
        "OBS_TYPE = 'divergence' #@param [\"accuracy\", \"loss\", \"divergence\"]\n",
        "REWARD_TYPE = 'accuracy' #@param [\"accuracy\", \"loss\"]\n",
        "OBS_INCLUDE_CLIENT_SIZE = 'no' #@param [\"no\", \"yes\"]\n",
        "AGG_STRATEGY = 'scale_sum' #@param [\"scale_sum\", \"scale_raw\"]\n",
        "N_RL_TRAINING_ROUNDS = 5 #@param {type:\"integer\"}\n",
        "N_BEST_RL_TRAINING_ROUNDS = 5 #@param {type:\"integer\"}\n",
        "N_MODEL_FL_EVAL_ROUNDS = 5 #@param {type:\"integer\"}\n",
        "N_FL_MODEL_EVAL_EPISODES = 2 #@param {type:\"integer\"}\n",
        "N_FL_LOCAL_TRAINING_ROUNDS = 1 #@param {type:\"integer\"}\n",
        "N_FL_CLIENTS = 5 #@param {type:\"integer\"}\n",
        "N_OPTUNA_TRIALS = 1 #@param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzlVJqZZB89f",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Dependencies + Imports\n",
        "\n",
        "# FL\n",
        "!pip install -q fedjax\n",
        "!pip install -q --upgrade -q jax jaxlib\n",
        "\n",
        "# RL\n",
        "!pip install -q gym==0.25.2\n",
        "!apt install -q swig cmake\n",
        "!pip install -q stable-baselines3[extra] box2d box2d-kengz\n",
        "!pip install -q optuna\n",
        "\n",
        "\n",
        "# Imports \n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import itertools\n",
        "import fedjax\n",
        "\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from scipy.special import softmax\n",
        "\n",
        "import gym\n",
        "import gym.spaces as spaces\n",
        "from gym.spaces import Discrete, MultiDiscrete\n",
        "import os\n",
        "from stable_baselines3 import TD3, DDPG, SAC, PPO\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
        "from stable_baselines3.common.noise import NormalActionNoise\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common import results_plotter\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecEnv, VecNormalize\n",
        "\n",
        "import optuna\n",
        "from optuna.pruners import MedianPruner\n",
        "from optuna.samplers import TPESampler, RandomSampler\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deYUah2F20uh"
      },
      "source": [
        "# RL & FL Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIIUqyLbEFPW"
      },
      "outputs": [],
      "source": [
        "class RL_Model:\n",
        "\n",
        "  def __init__(self, name):\n",
        "\n",
        "    if name == 'TD3':\n",
        "      self.model = TD3\n",
        "      self.name = 'TD3'\n",
        "      self.optuna_params = self.optuna_params_td3\n",
        "\n",
        "    elif name == 'DDPG':\n",
        "      self.model = DDPG\n",
        "      self.name = 'DDPG'\n",
        "      self.optuna_params = self.optuna_params_ddpg\n",
        "\n",
        "    elif name == 'SAC':\n",
        "      self.model = SAC\n",
        "      self.name = 'SAC'\n",
        "      self.optuna_params = self.optuna_params_sac\n",
        "\n",
        "    elif name == 'PPO':\n",
        "      self.model = PPO\n",
        "      self.name = 'PPO'\n",
        "      self.optuna_params = self.optuna_params_ppo\n",
        "\n",
        "\n",
        "  def optuna_params_td3(self, trial):\n",
        "    return  {\n",
        "      'gamma': trial.suggest_float('gamma', 0.1, 0.99),\n",
        "      'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.9),\n",
        "      'learning_starts': trial.suggest_int('learning_starts', 1e2, 1e4),\n",
        "      'tau': trial.suggest_float('tau', 5e-3, 0.9),\n",
        "      'target_policy_noise': trial.suggest_float('target_policy_noise', 1e-1, 5e-1),\n",
        "  }\n",
        "\n",
        "  def optuna_params_sac(self, trial):\n",
        "        return  {\n",
        "          'gamma': trial.suggest_float('gamma', 0.1, 0.99),\n",
        "          'learning_rate': trial.suggest_float('learning_rate', 1e-4, 0.9),\n",
        "          'learning_starts': trial.suggest_int('learning_starts', 1e2, 1e4),\n",
        "          'tau': trial.suggest_float('tau', 5e-3, 0.9),\n",
        "      }\n",
        "\n",
        "\n",
        "  def optuna_params_ddpg(self, trial):\n",
        "        return  {\n",
        "          'gamma': trial.suggest_float('gamma', 0.1, 0.99),\n",
        "          'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.9),\n",
        "          'learning_starts': trial.suggest_int('learning_starts', 1e2, 1e4),\n",
        "          'tau': trial.suggest_float('tau', 5e-3, 0.9)\n",
        "      }\n",
        "\n",
        "\n",
        "  def optuna_params_ppo(self, trial):\n",
        "        return  {\n",
        "          'gamma': trial.suggest_float('gamma', 0.1, 0.99),\n",
        "          'learning_rate': trial.suggest_float('learning_rate', 3e-4, 0.9),\n",
        "          'gae_lambda': trial.suggest_float('gae_lambda', 0.1, 0.99),\n",
        "          'clip_range': trial.suggest_float('clip_range', 0.1, 0.99),\n",
        "      }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJXcKmwuxrur"
      },
      "outputs": [],
      "source": [
        "class FL_Model:\n",
        "  def __init__(self, name):\n",
        "\n",
        "    if name == 'EMNIST':\n",
        "      self.train, self.test = fedjax.datasets.emnist.load_data(only_digits=False)\n",
        "      self.fl_model = fedjax.models.emnist.create_conv_model(only_digits=False)\n",
        "      self.accuracy_string = 'accuracy'\n",
        "      self.loss_string = 'loss'\n",
        "\n",
        "    elif name == 'Shakespeare':\n",
        "      self.train, self.test = fedjax.datasets.shakespeare.load_data()\n",
        "      self.fl_model = fedjax.models.shakespeare.create_lstm_model()\n",
        "      self.accuracy_string = 'accuracy_in_vocab'\n",
        "      self.loss_string = 'sequence_loss'\n",
        "\n",
        "    if OBS_TYPE == 'loss':\n",
        "          self.obs_metric_string = self.loss_string\n",
        "    else:\n",
        "        self.obs_metric_string = self.accuracy_string\n",
        "\n",
        "    if REWARD_TYPE == 'loss':\n",
        "          self.reward_metric_string = self.loss_string\n",
        "    else:\n",
        "        self.reward_metric_string = self.accuracy_string\n",
        "\n",
        "    self.name = name\n",
        "    self.rng = jax.random.PRNGKey(0)\n",
        "    self.init_params = self.fl_model.init(self.rng)\n",
        "    self.grad_fn = fedjax.model_grad(self.fl_model)\n",
        "    self.client_optimizer = fedjax.optimizers.sgd(0.1)\n",
        "\n",
        "    self.client_sampler = fedjax.client_samplers.UniformGetClientSampler(\n",
        "        self.train, num_clients=N_FL_CLIENTS, seed=1)\n",
        "    self.client_test_sampler = fedjax.client_samplers.UniformGetClientSampler(\n",
        "        self.test, num_clients=N_FL_CLIENTS, seed=1)\n",
        "    \n",
        "    self.batched_train_data = list(itertools.islice(\n",
        "        fedjax.padded_batch_federated_data(self.train, batch_size=128), 16))\n",
        "    self.batched_test_data = list(itertools.islice(\n",
        "         fedjax.padded_batch_federated_data(self.test, batch_size=128), 8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AUAw_nf6B8T"
      },
      "outputs": [],
      "source": [
        "RL_MODEL = RL_Model(RL_ALGORITHM_NAME)\n",
        "FL_MODEL = FL_Model(FL_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih-57Jaj7Fcw"
      },
      "source": [
        "# Directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5aF8xyV7ExP"
      },
      "outputs": [],
      "source": [
        "RL_MODELS_DIR = f\"/content/gdrive/MyDrive/FLRL/models/tests/{RL_MODEL.name}/{N_FL_CLIENTS}/{FL_MODEL.name}/{OBS_INCLUDE_CLIENT_SIZE}/{OBS_TYPE}/{REWARD_TYPE}/{AGG_STRATEGY}\"\n",
        "RL_LOG_DIR = f\"/content/gdrive/MyDrive/FLRL/logs/tests/{RL_MODEL.name}/{N_FL_CLIENTS}/{FL_MODEL.name}/{OBS_INCLUDE_CLIENT_SIZE}/{OBS_TYPE}/{REWARD_TYPE}/{AGG_STRATEGY}\"\n",
        "\n",
        "if not os.path.exists(RL_MODELS_DIR):\n",
        "    os.makedirs(RL_MODELS_DIR)\n",
        "\n",
        "if not os.path.exists(RL_LOG_DIR):\n",
        "    os.makedirs(RL_LOG_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db_tN_wuFX6_"
      },
      "source": [
        "# RL/FL Environmnent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qi4vWqpjF5AU"
      },
      "outputs": [],
      "source": [
        "class FLRLEnv(gym.Env):\n",
        "\n",
        "    metadata = {'render_modes': ['human']}\n",
        "  \n",
        "    def __init__(self, FL_MODEL, N_MODEL_FL_EVAL_ROUNDS, N_FL_LOCAL_TRAINING_ROUNDS, OBS_TYPE, REWARD_TYPE, OBS_INCLUDE_CLIENT_SIZE):\n",
        "        super().__init__()\n",
        "\n",
        "        self.action_space = spaces.Box(low=-1, high=1, shape=(N_FL_CLIENTS,), dtype=np.float32)\n",
        "\n",
        "        if OBS_INCLUDE_CLIENT_SIZE == 'yes':\n",
        "          self.observation_space = spaces.Box(low=0, high=np.inf, shape=(N_FL_CLIENTS, 2), dtype=np.float32)\n",
        "        else:\n",
        "          self.observation_space = spaces.Box(low=0, high=np.inf, shape=(N_FL_CLIENTS,), dtype=np.float32)\n",
        "\n",
        "    def client_update(self, init_params, client_dataset, client_rng, grad_fn):\n",
        "      opt_state = FL_MODEL.client_optimizer.init(init_params)\n",
        "      params = init_params\n",
        "      for batch in client_dataset.shuffle_repeat_batch(batch_size=10):\n",
        "        client_rng, use_rng = jax.random.split(client_rng)\n",
        "        grads = grad_fn(params, batch, use_rng)\n",
        "        opt_state, params = FL_MODEL.client_optimizer.apply(grads, opt_state, params)\n",
        "      delta_params = jax.tree_util.tree_map(lambda a, b: a - b,\n",
        "                                             init_params, params)\n",
        "      return len(client_dataset), params, delta_params\n",
        "\n",
        "\n",
        "    def run_one_fl_round(self, global_params):\n",
        "      sampled_clients_with_data = FL_MODEL.client_sampler.sample()\n",
        "      parameter_list = {}\n",
        "      metric_list = {}\n",
        "\n",
        "      original_params = global_params\n",
        "\n",
        "      for num in range(N_FL_LOCAL_TRAINING_ROUNDS):\n",
        "        for client_id, client_data, client_rng in sampled_clients_with_data:\n",
        "          num_samples, global_params, gradients = self.client_update(global_params, client_data, client_rng, FL_MODEL.grad_fn)\n",
        "\n",
        "          if (num == (N_FL_LOCAL_TRAINING_ROUNDS-1)):\n",
        "\n",
        "            if OBS_TYPE == 'divergence':\n",
        "              neg = fedjax.tree_util.tree_weight(original_params, -1)\n",
        "              sum = fedjax.tree_util.tree_add(neg, global_params)\n",
        "              metric = fedjax.tree_util.tree_l2_norm(sum)\n",
        "\n",
        "            else:\n",
        "              metric = (fedjax.evaluate_model(FL_MODEL.fl_model, global_params, FL_MODEL.batched_train_data))[FL_MODEL.obs_metric_string]\n",
        "            if OBS_INCLUDE_CLIENT_SIZE == 'yes':\n",
        "              metric_list[client_id] = (metric, num_samples)\n",
        "            else:\n",
        "              metric_list[client_id] = metric\n",
        "            parameter_list[client_id] = gradients\n",
        "          \n",
        "      metric_list = [metric_list[key] for key in sorted(metric_list.keys())]\n",
        "      parameter_list = [parameter_list[key] for key in sorted(parameter_list.keys())]\n",
        "\n",
        "      return metric_list, parameter_list\n",
        "\n",
        "    def aggregate_and_evaluate_parameters(self, old_global, parameter_list, scale_list):\n",
        "\n",
        "      if AGG_STRATEGY == 'scale_raw':\n",
        "        weighted_trees = [fedjax.tree_util.tree_weight(parameter_list[x], scale_list[x]) for x in range(len(scale_list))]\n",
        "        new_params = fedjax.tree_util.tree_sum(weighted_trees)\n",
        "\n",
        "      else:\n",
        "        scaled_list = [(parameter_list[x], scale_list[x]) for x in range(len(scale_list))]\n",
        "        new_params = fedjax.tree_util.tree_mean(scaled_list)\n",
        "\n",
        "      optimizer = fedjax.optimizers.sgd(learning_rate=0.9)\n",
        "      opt_state = optimizer.init(old_global)\n",
        "      _, new_params = optimizer.apply(new_params, opt_state, old_global)\n",
        "\n",
        "      metric = (fedjax.evaluate_model(FL_MODEL.fl_model, new_params, FL_MODEL.batched_train_data))[FL_MODEL.reward_metric_string]\n",
        "      return new_params, metric\n",
        "\n",
        "\n",
        "    def get_action(self, action):\n",
        "      action = ((np.array(action)+1.00001)/2)\n",
        "      if AGG_STRATEGY != 'scale_raw':\n",
        "        action = action*100\n",
        "      return action\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "  \n",
        "      self.steps += 1\n",
        "\n",
        "      action = self.get_action(action)\n",
        "      \n",
        "      self.global_params, metric = self.aggregate_and_evaluate_parameters(self.global_params, self.param_list, action)\n",
        "\n",
        "      if REWARD_TYPE == 'loss':\n",
        "        reward = - metric\n",
        "      else:\n",
        "        reward = metric\n",
        "        \n",
        "      metric_list, self.param_list = self.run_one_fl_round(self.global_params)\n",
        "      obs = metric_list\n",
        "\n",
        "      if (self.steps >= N_MODEL_FL_EVAL_ROUNDS):\n",
        "        done = True\n",
        "      else:\n",
        "        done = False\n",
        "      \n",
        "      info = {}\n",
        "      return obs, reward, done, info     \n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "      self.global_params = FL_MODEL.fl_model.init(FL_MODEL.rng)\n",
        "      metric_list, self.param_list = self.run_one_fl_round(self.global_params)\n",
        "      self.steps = 0         \n",
        "      return metric_list\n",
        "\n",
        "          \n",
        "    def render(self, action, reward, mode='human'):\n",
        "      if mode == 'human':\n",
        "        print(f\"action: {action}, reward = {reward}\") \n",
        "      else:\n",
        "        super().render(mode=mode) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rHOf5Y1FyRh"
      },
      "source": [
        "# Evalutation Flows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKeOZZPbhTQj"
      },
      "outputs": [],
      "source": [
        "def client_update(init_params, client_dataset, client_rng, grad_fn):\n",
        "  opt_state = FL_MODEL.client_optimizer.init(init_params)\n",
        "  params = init_params\n",
        "  for batch in client_dataset.shuffle_repeat_batch(batch_size=10):\n",
        "    client_rng, use_rng = jax.random.split(client_rng)\n",
        "    grads = grad_fn(params, batch, use_rng)\n",
        "    opt_state, params = FL_MODEL.client_optimizer.apply(grads, opt_state, params)\n",
        "  return len(client_dataset), params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDtFFVbUMLl3"
      },
      "outputs": [],
      "source": [
        "def get_action(action):\n",
        "\n",
        "    action = ((np.array(action)+1.00001)/2)\n",
        "\n",
        "    if AGG_STRATEGY != 'scale_raw':\n",
        "      action = action*100\n",
        "\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3AP6h9EI6Nt"
      },
      "outputs": [],
      "source": [
        "def run_fl_evaluation(epochs, init_params, rl_model):\n",
        "\n",
        "\n",
        "  global_params = init_params\n",
        "  original_params = init_params\n",
        "\n",
        "  accuracy_log_list = []\n",
        "  loss_log_list = []\n",
        "\n",
        "  for i in range(epochs):\n",
        "\n",
        "    sampled_clients_with_data = FL_MODEL.client_test_sampler.sample()\n",
        "    parameter_list = {}\n",
        "    metric_list = {}\n",
        "\n",
        "    for num in range(N_FL_LOCAL_TRAINING_ROUNDS):\n",
        "\n",
        "      for client_id, client_data, client_rng in sampled_clients_with_data:\n",
        "        num_samples, global_params = client_update(global_params, client_data, client_rng, FL_MODEL.grad_fn)\n",
        "\n",
        "        if (num == (N_FL_LOCAL_TRAINING_ROUNDS-1)):\n",
        "          \n",
        "          if OBS_TYPE == 'divergence':\n",
        "              neg = fedjax.tree_util.tree_weight(original_params, -1)\n",
        "              sum = fedjax.tree_util.tree_add(neg, global_params)\n",
        "              metric = fedjax.tree_util.tree_l2_norm(sum)\n",
        "\n",
        "          else:\n",
        "              metric = (fedjax.evaluate_model(FL_MODEL.fl_model, global_params, FL_MODEL.batched_test_data))[FL_MODEL.obs_metric_string]\n",
        "         \n",
        "          if OBS_INCLUDE_CLIENT_SIZE == 'yes':\n",
        "            metric_list[client_id] = (metric, num_samples)\n",
        "          else:\n",
        "            metric_list[client_id] = metric\n",
        "          parameter_list[client_id] = global_params\n",
        "\n",
        "    metric_list = [metric_list[key] for key in sorted(metric_list.keys())]\n",
        "    parameter_list = [parameter_list[key] for key in sorted(parameter_list.keys())]\n",
        "\n",
        "    scale_list, _ = rl_model.predict(metric_list)\n",
        "    scale_list = get_action(scale_list)\n",
        "\n",
        "    if AGG_STRATEGY == 'scale_raw':\n",
        "        weighted_trees = [fedjax.tree_util.tree_weight(parameter_list[x], scale_list[x]) for x in range(len(scale_list))]\n",
        "        global_params = fedjax.tree_util.tree_sum(weighted_trees)\n",
        "    else:\n",
        "      scaled_list = [(parameter_list[x], scale_list[x]) for x in range(len(scale_list))]\n",
        "      global_params = fedjax.tree_util.tree_mean(scaled_list)\n",
        "\n",
        "    eval_metrics = (fedjax.evaluate_model(FL_MODEL.fl_model, global_params, FL_MODEL.batched_test_data))\n",
        "    accuracy, loss = eval_metrics[FL_MODEL.accuracy_string], eval_metrics[FL_MODEL.loss_string]\n",
        "    accuracy_log_list.append(accuracy)\n",
        "    loss_log_list.append(loss)\n",
        "\n",
        "  return  accuracy_log_list, loss_log_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vi8khbK_vSlN"
      },
      "outputs": [],
      "source": [
        "def run_fedvag(epochs, init_params):\n",
        "\n",
        "\n",
        "  global_params = init_params\n",
        "  accuracy_log_list = []\n",
        "  loss_log_list = []\n",
        "\n",
        "\n",
        "  for i in range(epochs):\n",
        "\n",
        "    sampled_clients_with_data = FL_MODEL.client_test_sampler.sample()\n",
        "    client_updates = []\n",
        "\n",
        "    for num in range(N_FL_LOCAL_TRAINING_ROUNDS):\n",
        "\n",
        "      for client_id, client_data, client_rng in sampled_clients_with_data:\n",
        "        num_samples, global_params = client_update(global_params, client_data, client_rng, FL_MODEL.grad_fn)\n",
        "\n",
        "        if (num == (N_FL_LOCAL_TRAINING_ROUNDS-1)):\n",
        "           client_updates.append((global_params, num_samples))\n",
        "\n",
        "\n",
        "    global_params = fedjax.tree_util.tree_mean(client_updates)\n",
        "\n",
        "    eval_metrics = (fedjax.evaluate_model(FL_MODEL.fl_model, global_params, FL_MODEL.batched_test_data))\n",
        "    accuracy, loss = eval_metrics[FL_MODEL.accuracy_string], eval_metrics[FL_MODEL.loss_string]\n",
        "    accuracy_log_list.append(accuracy)\n",
        "    loss_log_list.append(loss)\n",
        "\n",
        "  return  accuracy_log_list,loss_log_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_seV3Eb7zty"
      },
      "source": [
        "# Helpers: Optuna, Callback, Initalise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nH1TRQrKJnnU"
      },
      "outputs": [],
      "source": [
        "def create_rl_model_and_environment(train_test, params):\n",
        "\n",
        "  if train_test == 'training':\n",
        "    n_envs = 1\n",
        "    environment = [lambda: Monitor(FLRLEnv(FL_MODEL, N_MODEL_FL_EVAL_ROUNDS, N_FL_LOCAL_TRAINING_ROUNDS, OBS_TYPE, REWARD_TYPE, OBS_INCLUDE_CLIENT_SIZE), f\"{RL_LOG_DIR}\")  for i in range(n_envs)]\n",
        "    environment = DummyVecEnv(environment)\n",
        "    environment= VecNormalize(environment)\n",
        " \n",
        "    if params != None:\n",
        "      if (RL_MODEL.name != 'TD3' and  RL_MODEL.name != 'DDPG'):\n",
        "        rl_model = (RL_MODEL.model)(\"MlpPolicy\", environment, verbose=1, **params)\n",
        "      else:\n",
        "        action_noise = NormalActionNoise(mean=np.zeros(N_FL_CLIENTS), sigma=0.1 * np.ones(N_FL_CLIENTS))\n",
        "        rl_model = (RL_MODEL.model)(\"MlpPolicy\", environment, action_noise=action_noise, verbose=1, **params)\n",
        "    else:\n",
        "      if (RL_MODEL.name != 'TD3' and  RL_MODEL.name != 'DDPG'):\n",
        "        rl_model = (RL_MODEL.model)(\"MlpPolicy\", environment, verbose=1)\n",
        "      else:\n",
        "        action_noise = NormalActionNoise(mean=np.zeros(N_FL_CLIENTS), sigma=0.1 * np.ones(N_FL_CLIENTS))\n",
        "        rl_model = (RL_MODEL.model)(\"MlpPolicy\", environment, action_noise=action_noise, verbose=1)\n",
        "\n",
        "  else:\n",
        "    n_envs = 1\n",
        "    environment = [lambda: Monitor(FLRLEnv(FL_MODEL, N_MODEL_FL_EVAL_ROUNDS, N_FL_LOCAL_TRAINING_ROUNDS, REWARD_TYPE, OBS_TYPE, OBS_INCLUDE_CLIENT_SIZE), f\"{RL_LOG_DIR}\")  for i in range(n_envs)]\n",
        "    environment = DummyVecEnv(environment)\n",
        "    environment= VecNormalize(environment, norm_reward=False)\n",
        "\n",
        "    rl_model = None\n",
        "    \n",
        "  return rl_model, environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM3irT6e-1fj"
      },
      "outputs": [],
      "source": [
        "class Objective:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.best_model = None\n",
        "    self.best_env = None\n",
        "    self.best_reward = None\n",
        "    self.model = None\n",
        "    self.env = None\n",
        "    self.reward = None\n",
        "\n",
        "\n",
        "  def __call__(self, trial):\n",
        "\n",
        "    model, env = create_rl_model_and_environment('training', RL_MODEL.optuna_params(trial))\n",
        "    model.learn(total_timesteps=N_RL_TRAINING_ROUNDS)\n",
        "    self.model = model\n",
        "    self.env = env\n",
        "    mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=N_FL_MODEL_EVAL_EPISODES)\n",
        "    self.reward = mean_reward\n",
        "\n",
        "    return 1 * mean_reward\n",
        "\n",
        "  def callback(self, study, trial):\n",
        "        if study.best_trial == trial:\n",
        "            print(\"Saving new best model and env\")\n",
        "            self.best_model = self.model\n",
        "            self.best_env = self.env\n",
        "            self.best_reward = self.reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3WRLuXT85ys"
      },
      "outputs": [],
      "source": [
        "class SaveBestModel(BaseCallback):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(SaveBestModel, self).__init__()\n",
        "        self.check_freq = N_MODEL_FL_EVAL_ROUNDS\n",
        "        self.log_dir = RL_LOG_DIR\n",
        "        self.save_path = os.path.join(RL_MODELS_DIR, 'best_model')\n",
        "        self.best_mean_reward = -np.inf\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "      \n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "\n",
        "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
        "\n",
        "          if len(x) > 0:\n",
        "              mean_reward = np.mean(y[-N_MODEL_FL_EVAL_ROUNDS:])\n",
        "              print(f\"Best mean episode reward: {self.best_mean_reward:.2f} - Latest mean episode reward: {mean_reward:.2f}\")\n",
        "\n",
        "              if mean_reward > self.best_mean_reward:\n",
        "                  self.best_mean_reward = mean_reward\n",
        "                  print(f\"Saving new best model  {mean_reward} to {self.save_path}.zip\")\n",
        "                  self.model.save(self.save_path)\n",
        "                  \n",
        "        return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwAuTiCfJg4-"
      },
      "outputs": [],
      "source": [
        "def fl_final_evaluation_plot(title, model_list, fed_avg_list):\n",
        " \n",
        "  fig = plt.figure(title,figsize=(16,10), dpi= 200)\n",
        "  plt.plot(range(1,N_MODEL_FL_EVAL_ROUNDS+1), model_list, 'g', label='RL Model')\n",
        "  plt.plot(range(1,N_MODEL_FL_EVAL_ROUNDS+1), fed_avg_list, 'b', label='FedAvg')\n",
        "  plt.xlabel('FL Round')\n",
        "  plt.ylabel(title)\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  return plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TnAI1xFJq_L"
      },
      "outputs": [],
      "source": [
        "def train_main():\n",
        "  # Optimse RL Algo\n",
        "  print(\"---------- \\RL Algo Train: Tuning RL Model with Optuna\")\n",
        "  objective = Objective()\n",
        "  sampler = RandomSampler()\n",
        "  pruner = MedianPruner()\n",
        "  study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
        "  study.optimize(objective, n_trials=N_OPTUNA_TRIALS, n_jobs=1, callbacks=[objective.callback])\n",
        "  best_reward = objective.best_reward\n",
        "\n",
        "  # Check Default Param Version\n",
        "  print(\"---------- \\RL Algo Train: Checking Default Param option\")\n",
        "  rl_model, environment = create_rl_model_and_environment('training', None)\n",
        "  rl_model.learn(total_timesteps=N_RL_TRAINING_ROUNDS)\n",
        "  mean_reward, _ = evaluate_policy(rl_model, environment, n_eval_episodes=N_FL_MODEL_EVAL_EPISODES)\n",
        "\n",
        "  # Choose Default v Optuna:\n",
        "  if (mean_reward) < best_reward:\n",
        "    rl_model, environment = create_rl_model_and_environment('training', study.best_params)\n",
        "    print(f\"Using the Optuna Model with params {study.best_params}\")\n",
        "  else:\n",
        "    print(\"Using the Default Params Model\")\n",
        "\n",
        "  # Extended Train best RL Model\n",
        "  print(\"---------- \\RL Algo Train: Extended Training Best RL Model\")\n",
        "  callback = SaveBestModel()\n",
        "  rl_model.learn(total_timesteps=N_BEST_RL_TRAINING_ROUNDS, callback=callback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9ODOSzTzr8_"
      },
      "outputs": [],
      "source": [
        "def evaluate_main():\n",
        "\n",
        "  # Load best RL Model from training:\n",
        "  model_path = f\"{RL_MODELS_DIR}/best_model.zip\"\n",
        "  __ , test_environment = create_rl_model_and_environment('test', None)\n",
        "  best_rl_model = (RL_MODEL.model).load(model_path, env=test_environment)\n",
        "\n",
        "  # Evaluate Best Model in FL Test.\n",
        "  print(\"---------- \\nRL Algo Evaluation: Evaluating Best RL Model in FL Setting\")\n",
        "  fl_accuracy_list, fl_loss_list = np.mean([run_fl_evaluation(N_MODEL_FL_EVAL_ROUNDS, FL_MODEL.init_params, best_rl_model) for i in range(N_FL_MODEL_EVAL_EPISODES)], 0)\n",
        "\n",
        "  # Get FedAVG Comparison Results.\n",
        "  print(\"---------- \\nRL Algo Evaluation: Evaluating FEDAVG for FL Setting\")\n",
        "  fedavg_accuracy_list, fedavg_loss_list = np.mean([run_fedvag(N_MODEL_FL_EVAL_ROUNDS, FL_MODEL.init_params) for i in range(N_FL_MODEL_EVAL_EPISODES)], 0)\n",
        "\n",
        "  outcome_string = f\"Best RL Model in FL evaluation is: {fl_accuracy_list[-1]}, FedAVG acuracy is: {fedavg_accuracy_list[-1]}\"\n",
        "\n",
        "  return fl_accuracy_list, fedavg_accuracy_list, fl_loss_list, fedavg_loss_list, outcome_string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2PfOg6-dnAD"
      },
      "source": [
        "# Run: Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kt5Qc1X8JvgC"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "train_main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcbCadM20lyg"
      },
      "outputs": [],
      "source": [
        "# View Training Performance:\n",
        "results_plotter.plot_results([RL_LOG_DIR], N_BEST_RL_TRAINING_ROUNDS, results_plotter.X_TIMESTEPS, \"Results\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhLxRMo5dX1z"
      },
      "source": [
        "# Run: Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1l13E3H20Am9"
      },
      "outputs": [],
      "source": [
        "fl_accuracy_list, fedavg_accuracy_list, fl_loss_list, fedavg_loss_list, outcome_string = evaluate_main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkLjzzmac5Ww"
      },
      "outputs": [],
      "source": [
        "# Show Results:\n",
        "acc_plot = fl_final_evaluation_plot ('Accuracy', fl_accuracy_list,  fedavg_accuracy_list)\n",
        "acc_plot.show()\n",
        "loss_plot = fl_final_evaluation_plot ('Loss', fl_loss_list,  fedavg_loss_list)\n",
        "loss_plot.show()\n",
        "\n",
        "print(outcome_string)\n",
        "print(RL_MODELS_DIR)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}